<============masking both weights and gradients for retrain<============all masking statistics<============testing sparsity before retrain<===sparsity type is block_prune_filter<===layers to be pruned are [0.95, 0.95]sparsity at layer wh.0.weight is 0.9942157451923077sparsity at layer wh.1.weight is 0.9971799850463867sparsity at layer uh.0.weight is 0.9969053268432617sparsity at layer uh.1.weight is 0.9942359924316406sparsity at layer wz.0.weight is 0.9935396634615384sparsity at layer wz.1.weight is 0.9969377517700195sparsity at layer uz.0.weight is 0.9970893859863281sparsity at layer uz.1.weight is 0.9968833923339844sparsity at layer wr.0.weight is 0.9936648637820513sparsity at layer wr.1.weight is 0.9971284866333008sparsity at layer ur.0.weight is 0.9970884323120117sparsity at layer ur.1.weight is 0.9970073699951172overal compression rate is 301.28280949528704Decoding TIMIT_test output out_dnn2%WER 25.5 | 192 7215 | 77.7 16.8 5.5 3.3 25.6 100.0 | -0.400 | /home/dongpe/RNN/temp2/exp/TIMIT_GRU_mfcc/decode_TIMIT_test_out_dnn2/score_4/ctm_39phn.filt.sys-----Generating output files and plots ...OK