<============masking both weights and gradients for retrain<============all masking statistics<============testing sparsity before retrain<===sparsity type is block_prune_filter<===layers to be pruned are [0.875, 0.875]sparsity at layer wh.0.weight is 0.9830729166666666sparsity at layer wh.1.weight is 0.9916009902954102sparsity at layer uh.0.weight is 0.9910602569580078sparsity at layer uh.1.weight is 0.9838685989379883sparsity at layer wz.0.weight is 0.9815204326923077sparsity at layer wz.1.weight is 0.9910373687744141sparsity at layer uz.0.weight is 0.9912700653076172sparsity at layer uz.1.weight is 0.9911403656005859sparsity at layer wr.0.weight is 0.9826472355769231sparsity at layer wr.1.weight is 0.9914731979370117sparsity at layer ur.0.weight is 0.9915485382080078sparsity at layer ur.1.weight is 0.9912347793579102overal compression rate is 103.8363302513065Decoding TIMIT_test output out_dnn2%WER 23.2 | 192 7215 | 79.9 14.9 5.2 3.3 23.4 99.5 | -0.558 | /home/dongpe/RNN/temp0/pytorch-kaldi/exp/TIMIT_GRU_mfcc/decode_TIMIT_test_out_dnn2/score_4/ctm_39phn.filt.sys-----Generating output files and plots ...OK