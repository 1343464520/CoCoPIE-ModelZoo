<============masking both weights and gradients for retrain<============testing sparsity before retrain<===sparsity type is block_prune_filter<===layers to be pruned are [0.9, 0.9]sparsity at layer wh.0.weight is 0.908203125sparsity at layer wh.1.weight is 0.8994140625sparsity at layer uh.0.weight is 0.8994140625sparsity at layer uh.1.weight is 0.8994140625sparsity at layer wz.0.weight is 0.9100060096153846sparsity at layer wz.1.weight is 0.8994140625sparsity at layer uz.0.weight is 0.8994140625sparsity at layer uz.1.weight is 0.8994140625sparsity at layer wr.0.weight is 0.9070763221153846sparsity at layer wr.1.weight is 0.8994140625sparsity at layer ur.0.weight is 0.8994140625sparsity at layer ur.1.weight is 0.8994140625overal compression rate is 9.952929487960558Decoding TIMIT_test output out_dnn2%WER 18.8 | 192 7215 | 83.3 12.3 4.4 2.3 18.9 99.0 | -0.542 | /home/dongpe/RNN/temp/pytorch-kaldi/exp/TIMIT_GRU_mfcc/decode_TIMIT_test_out_dnn2/score_5/ctm_39phn.filt.sys-----Generating output files and plots ...OK